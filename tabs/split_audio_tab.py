import os
from typing import List, Union

import gradio as gr

from utils.preprocess import extract_audio_chunks_from_video
from backends import get_asr_backend
from core.transcription import StreamingTranscriber


def load_audio(selected_row):
    # selected_row æ˜¯ä¸€å€‹åŒ…å«è©²åˆ—æ‰€æœ‰å€¼çš„åˆ—è¡¨
    # ä¾‹å¦‚ï¼š['audios/sample.wav', 'ç¯„ä¾‹éŸ³è¨Š 1 (440 Hz)']
    return selected_row[3], selected_row[4]


def ui_stream_process(
    video,
    chunk_sec,
    overlap_sec,
    max_utt_sec,
    stall_ms,
    min_utt_sec,
    partial_min_interval,
):
    """
    Gradio äº‹ä»¶å‡½å¼ï¼ˆgeneratorï¼‰ï¼šä¸Šå‚³å½±ç‰‡å¾Œç«‹å³é–‹å§‹ã€Œä¸²æµå¼ã€è½‰å¯«ã€‚
    """
    streaming_asr = get_asr_backend("sherpa-onnx")

    transcriber = StreamingTranscriber(
        streaming_asr,
        chunk_sec=float(chunk_sec),
        overlap_sec=float(overlap_sec),
        max_utt_sec=float(max_utt_sec),
        stall_ms=int(stall_ms),
        min_utt_sec=float(min_utt_sec),
        partial_min_interval=float(partial_min_interval),
    )

    samples: List[List[Union[int, float, str]]] = []
    idx = 1
    latest_audio = None

    # åˆæ¬¡å›å‚³ç©ºè³‡æ–™é›†ï¼Œé¿å… UI é‚„æ²’è³‡æ–™æ™‚å ±å‹åˆ¥
    yield "", gr.Dataset(samples=[]), None

    for pcm in extract_audio_chunks_from_video(video, chunk_sec=float(chunk_sec)):
        partial, finalized_segment = transcriber.stream_transcribe(pcm)
 
        # å³æ™‚æ›´æ–°åˆæ­¥è¾¨è­˜çµæœ
        if partial:
            yield partial, None, latest_audio

        # ç•¶ä¸€å€‹æ®µè½çµæŸæ™‚ï¼Œæ›´æ–° Dataset
        if finalized_segment:
            t0, t1, text, audio_path = finalized_segment
            row = [
                idx,
                f"{t0:.2f}",
                f"{t1:.2f}",
                text,
                audio_path,
            ]
            samples.append(row)
            idx += 1
            latest_audio = audio_path
            yield partial or "", gr.update(samples=samples), latest_audio


def create_split_audio_tab():
    with gr.TabItem("Split Audio Tab"):
        with gr.Row():
            with gr.Column(scale=2):
                video = gr.Video(
                    sources=["upload"], label="ğŸ“‚ ä¸Šå‚³æª”æ¡ˆ", interactive=True
                )
                with gr.Accordion("é€²éšåƒæ•¸", open=False):
                    chunk_sec = gr.Slider(0.4, 3.0, value=1.2, step=0.1, label="å¤–éƒ¨è¼¸å…¥ chunkï¼ˆç§’ï¼‰")
                    overlap_sec = gr.Slider(0.0, 1.2, value=0.25, step=0.05, label="å…§éƒ¨æ»‘çª—é‡ç–Šï¼ˆç§’ï¼‰")
                    max_utt_sec = gr.Slider(2, 20, value=6.0, step=0.5, label="æœ€é•·ä¸€å¥ï¼ˆç§’ï¼‰")
                    stall_ms = gr.Slider(200, 2000, value=900, step=50, label="ä¹…æœªè®Šå¼·åˆ¶åˆ‡æ®µï¼ˆæ¯«ç§’ï¼‰")
                    partial_min_interval = gr.Slider(0.1, 1.0, value=0.25, step=0.05, label="åˆæ­¥çµæœæ›´æ–°é–“éš”ï¼ˆç§’ï¼‰")
                    min_utt_sec = gr.Slider(0.0, 5.0, value=2.0, step=0.1, label="åœé “è§¸ç™¼çš„æœ€å°å¥é•·")

                transcribe_btn = gr.Button("é–‹å§‹è™•ç†")

            with gr.Column(scale=4):
                partial = gr.Textbox(label="å³æ™‚è¾¨è­˜çµæœ", lines=4)
                player = gr.Audio(label="ç•¶å‰æ’­æ”¾çš„éŸ³è¨Š")

                segments = gr.Dataset(
                    components=[
                        gr.Number(label="#", precision=0, visible=False),
                        gr.Textbox(label="é–‹å§‹(s)", visible=False),
                        gr.Textbox(label="çµæŸ(s)", visible=False),
                        gr.Textbox(label="è¾¨è­˜æ–‡å­—", visible=False),
                        gr.Audio(label="Audio", type="filepath", visible=False),
                    ],
                    headers=["#", "é–‹å§‹", "çµæŸ", "æ–‡å­—", "éŸ³è¨Š"],
                    samples=[],
                    label="å·²åˆ‡åˆ†éŸ³è¨Šæ®µè½ (å¯ç›´æ¥åœ¨è¡¨æ ¼ä¸­æ’­æ”¾)",
                )

        # --- Event Listeners ---
        transcribe_btn.click(
            fn=ui_stream_process,
            inputs=[
                video,
                chunk_sec,
                overlap_sec,
                max_utt_sec,
                stall_ms,
                min_utt_sec,
                partial_min_interval,
            ],
            outputs=[partial, segments, player],
        )

        segments.click(
            fn=load_audio,
            inputs=[segments],
            outputs=[partial, player]
        )